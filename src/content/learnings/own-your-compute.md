---
title: "Own your compute — consumer GPUs are the new competitive advantage"
date: "2026-02-05"
source: "Building marl0.space infrastructure"
tags: ["infrastructure", "GPU", "local-LLM", "independence"]
---

Dual RTX 5090s running Ollama can serve 32B parameter models that would have required a data center two years ago. Our entire classification pipeline — thousands of entities — runs at zero marginal cost.

When you own the hardware: no API rate limits, no per-token costs, no vendor lock-in, no one deciding what you can and can't do with the models. The up-front cost of two GPUs pays for itself in weeks of heavy API usage.

This isn't just about saving money. It's about independence. When your infrastructure is someone else's API, your capabilities are bounded by their pricing, their policies, and their uptime.
