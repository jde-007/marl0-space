---
title: "Personal Infrastructure"
tagline: "Not just knowing how to use tools ‚Äî owning the infrastructure."
category: "operate"
icon: "üñ•Ô∏è"
order: 7
stat: "32B"
statLabel: "parameter models running locally"
---

- **Mac Mini M4** running all application services
- **Dual RTX 5090 GPU server** for local LLM inference ‚Äî zero API costs
- **Cloudflare Tunnel** providing zero-trust access to 14+ services
- **OpenClaw AI** running 24/7 with persistent memory and self-healing restarts
- **Ollama** serving 32B parameter models that would have required a data center two years ago
- Own your compute
- When you run models locally, you don't have API rate limits
- You don't have per-token costs
- And you don't have a vendor deciding what you can and can't do
- Consumer GPUs in 2026 can run models that would have required a data center in 2023
- This infrastructure isn't a hobby ‚Äî it's a competitive advantage
